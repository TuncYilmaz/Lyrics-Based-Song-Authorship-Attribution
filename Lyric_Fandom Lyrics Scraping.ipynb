{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The website https://lyrics.fandom.com consists of artist pages, that are contained in index pages that have the following URL format:\n",
    "https://lyrics.fandom.com/wiki/Category:Artists_A?from=Aa <br>\n",
    "Therefore all the index page URLs can be generated with the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\", \"i\", \"j\", \"k\", \"l\", \"m\", \"n\", \"o\", \"p\", \"q\", \"r\", \"s\", \"t\", \"u\", \"v\", \"w\", \"x\", \"y\", \"z\"]\n",
    "\n",
    "category_artist_urls = []\n",
    "for letter in alphabet:\n",
    "    for second_letter in alphabet:\n",
    "        category_artist_urls.append('https://lyrics.fandom.com/wiki/Category:Artists_'+letter.upper()+\"?from=\"+letter.upper()+second_letter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 676 different artist index pages.\n",
      "An example artist index page URL is: https://lyrics.fandom.com/wiki/Category:Artists_J?from=Ja\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\", len(category_artist_urls), \"different artist index pages.\")\n",
    "print(\"An example artist index page URL is:\", category_artist_urls[234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import BeautifulSoup and urllib for web scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import urllib.request as urllib2\n",
    "except ImportError:\n",
    "    import urllib2 \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of artists that are contained in certain index pages exceed the number that can be fit into a single page. In such cases, the remaining artist names are presented in other index pages represented by random URLs formed by the first artist name in those pages (e.g. https://lyrics.fandom.com/wiki/Category:Artists_A?from=Above+%26+Beyond) instead of using the standart URL format for index pages (such as https://lyrics.fandom.com/wiki/Category:Artists_J?from=Ja). <br> \n",
    "The following code tries to spot such exceptions, and retrieves the random URLs, so that **all** artist index pages are recorded into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://lyrics.fandom.com/wiki/Category:Artists_D?from=Dutch+Kills page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_E?from=Evolve page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_F?from=Fuzz+Fuzz+Machine page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_G?from=Gu%C3%A9na+LG+%26+Amir+Afargan page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_J?from=Junk+Science page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_L?from=Lusine page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_M?from=Mystik+Spiral page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_U?from=Unleashed+Power page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_W?from=Wowaka page not accessed\n",
      "https://lyrics.fandom.com/wiki/Category:Artists_Y?from=Your+Shapeless+Beauty page not accessed\n"
     ]
    }
   ],
   "source": [
    "# first find the index pages that have the exception described above, and store them in a list called exceptions\n",
    "exceptions = []\n",
    "for url in category_artist_urls:\n",
    "    try:\n",
    "        page = urllib2.urlopen(url)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        # check the page content to see if there is an html class that contains the 'next' button, indicating that\n",
    "        # this index page overflows to subsequent index pages containing other artists with the same initial letter\n",
    "        content = soup.find_all('a', {\"class\": \"category-page__pagination-next wds-button wds-is-secondary\"})\n",
    "        if content == []: # means that there is no 'next' button in the page, so there is no exception\n",
    "            continue\n",
    "        else: # add the exceptional URL to the list\n",
    "            exceptions.append(url)\n",
    "    except urllib2.HTTPError:\n",
    "        pass\n",
    "    \n",
    "# then, use a handler function to go through all the index pages with exceptional URLs, \n",
    "# and collect the artist page URLs stored in the html code of those index pages\n",
    "\n",
    "additional_category_pages = [] # a list to collect those additional pages\n",
    "def exception_getter(url): # the handler function for retrieving additional random category page URLs\n",
    "    try:\n",
    "        page = urllib2.urlopen(url)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        \n",
    "        #find the url that is attached to the 'next' button in the page\n",
    "        content = soup.find_all('a', {\"class\": \"category-page__pagination-next wds-button wds-is-secondary\"})\n",
    "        next_url = content[0]['href']\n",
    "        \n",
    "        # find the starting two-letters of the first artist of the next page\n",
    "        index_start_next = next_url.find(\"from\")\n",
    "        first_two_letters_next = next_url[index_start_next+5:index_start_next+7]\n",
    "        \n",
    "        # also record the first two letters of the first artist in the current page\n",
    "        index_start_now = url.find('from')\n",
    "        first_two_letters_now = url[index_start_now+5:index_start_now+7]\n",
    "            \n",
    "        # apply the function recursively until we reach to a url that has a first artist starting with different two-letters\n",
    "        if first_two_letters_next == first_two_letters_now:\n",
    "            additional_category_pages.append(next_url)\n",
    "            exception_getter(next_url)\n",
    "        else:\n",
    "            return\n",
    "    except:\n",
    "        print(url,\"page not accessed\")\n",
    "\n",
    "\n",
    "# finally, go to each of those index pages, follow until the page doesn't contain any artist that starts with \n",
    "# the corresponding character bigram, and collect all the random URLs generated using the handler function above\n",
    "for exception in exceptions:\n",
    "    exception_getter(exception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 965 artist pages to be scraped\n"
     ]
    }
   ],
   "source": [
    "# add the additional category pages to the initial list to get the complete url set:\n",
    "artist_page_urls = list(set(category_artist_urls + additional_category_pages))\n",
    "print(\"There are\", len(artist_page_urls), \"artist pages to be scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the variables to a pickle file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "def writePickle( Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "def readPickle(fname):\n",
    "    filename = \"pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writePickle(artist_page_urls, \"Artist_Collection_Pages\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each artist collection (index) page, there are multiple artists listed with a link to their own artist pages. We store each of these in a dictionary where the artist name maps to its link in the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_page_urls = readPickle(\"Artist_Collection_Pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "artist_to_url = {}\n",
    "for url in artist_page_urls:\n",
    "    try:\n",
    "        page = urllib2.urlopen(url)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        artists = soup.find_all('a', {\"class\": \"category-page__member-link\"})\n",
    "        for item in artists:\n",
    "            artist_to_url[item['title']] = item['href']\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write the artist page urls dictionary to a json file for later use\n",
    "import json\n",
    "with open('Artist_Pages.txt', 'w') as outfile:\n",
    "    json.dump(artist_to_url, outfile)\n",
    "    \n",
    "# also save it to a pickle file\n",
    "writePickle(artist_to_url, \"Artist_Pages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 79280 different artists with corresponding links to their artist pages are stored\n"
     ]
    }
   ],
   "source": [
    "print(\"Now there are\", len(artist_to_url), \"different artists with corresponding links to their artist pages are stored\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final move before getting all the lyrics is to generate a placeholder dictionary for all the songs to be retrieved from the website. This dictionary has the following format: <br> <br>\n",
    "{'Artist1 Name' : <br>\n",
    "        &emsp;[Genre, URL_of_Artist, <br>\n",
    "         &emsp;&emsp;{'Album1_name': <br>\n",
    "           &emsp;&emsp;&emsp;[URL_of_Album, <br>\n",
    "           &emsp;&emsp;&emsp;&emsp;{'song1': [lyrics, year, URL_of_Song, Song_ID],<br>\n",
    "           &emsp;&emsp;&emsp;&emsp;'song2': [ ], <br> \n",
    "           &emsp;&emsp;&emsp;&emsp;...., <br> \n",
    "           &emsp;&emsp;&emsp;&emsp;'songN': [ ] }<br>\n",
    "           &emsp;&emsp;&emsp;] <br>\n",
    "          &emsp;&emsp;'Album2_name':....,<br>\n",
    "          &emsp;&emsp;...., <br>\n",
    "          &emsp;&emsp;'AlbumN_name':....<br>\n",
    "         &emsp;&emsp;}<br>\n",
    "        &emsp;],<br>\n",
    "     'Artist2 Name': ...,<br>\n",
    "     'ArtistN Name':...<br>\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a new dictionary for lyrics\n",
    "lyrics_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# there are certain genres that we don't want to cover due to several reasons\n",
    "# 'classical music' for instance, tend to have no lyrics, and we don't want to spend time scraping them\n",
    "# other genres are in the blacklist due to several reasons such as ambiguity, foreign language, scarcity...\n",
    "# use the following list to avoid retrieving songs of artists belonging to unwanted genres\n",
    "avoid_genre_list = ['Acoustic', 'Aggrotech', 'Alternative Country', 'Alternative Metal', 'Americana', 'Andean Music', 'Australian Hip Hop', 'Avant-garde', 'Ballad', 'Brazilian Rock', 'Canterbury', 'Celtic', 'Celtic Folk', 'Chanson', \"Children's Music\", 'Christian Hip Hop', 'Christian Metal', 'Christian Rock', 'Comedy', 'Contemporary Christian', 'Crust Punk', 'Dance', 'Dark Wave', 'Death Doom', 'Deathcore', 'Disco', 'Downtempo', 'EBM', 'Electronica', 'Electropop', 'Experimental Hip Hop', 'Experimental Pop', 'Experimental Rock', 'Extreme Metal', 'Forró', 'Freestyle', 'Goregrind', 'Gospel', 'Gothic Metal', 'Gothic Rock', 'Grindcore', 'Hardcore Punk', 'Horror Punk', 'Horrorcore', 'House', 'Industrial', 'Industrial Metal', 'J-Pop', 'J-Rock', 'Latin', 'Melodic Death Metal', 'Melodic Metalcore', 'Metalcore', 'Morna', 'New Jack Swing', 'New Wave', 'Noise', 'Pop Punk', 'Post-Hardcore', 'Power Metal', 'Progressive House', 'Progressive Metal', 'Punk Rock', 'Rockabilly', 'Roots Rock','Singer-Songwriter','Sludge Metal','Spanish Rock','Spoken Word','Stoner Rock','Synthpop', 'A Cappella',\n",
    " 'Adult Contemporary','Afropop','Ambient','Austropop','Avant-garde Metal','Axé','Bachata','Baroque Pop','Black Metal',\n",
    " 'Blackgaze','Blue-Eyed Soul','Bluegrass','Blues Revival','Blues Rock','Bossa Nova','British Blues','Bubblegum Pop','Canadian Folk',\n",
    " 'Celtic Rock','Christian','Classical','Comedy Rock','Contemporary Folk','Country Rock','Crossover Thrash','Crunk',\n",
    " 'Cumbia','Dance Punk','Dark Electro','Deathrock','Deutschrock','Doom Metal','Drone Music','Drum And Bass','Dubstep',\n",
    " 'Easy Listening','Electronic Rock','Emo','Eurobeat','Eurodance','Fado','Flamenco','Folk Metal','Folk Punk','Folk Rock',\n",
    " 'Freak Folk','French Hip Hop','Garage Rock','Glam Metal','Glam Rock','Gothic','Groove Metal','Happy Hardcore','Industrial Rock',\n",
    " 'Italian Folk','Italo Disco','Jazz Fusion','K-Pop','Kirtan','Latin Pop','Lo-Fi','MPB','Medieval','Melodic Hardcore',\n",
    " 'Neo-Psychedelia','Neofolk','Nerdcore Hip Hop','Noise Rock','Nu Metal','Nu Metalcore','Nueva Canción','Oi-Punk',\n",
    " 'Pagan Metal','French Pop','Pagode','Persian','Neue Deutsche Welle','Norteño','Pachanga','Post-Punk','Progressive Metalcore',\n",
    " 'Psychedelic','RAC','Raggamuffin','Rap Rock','Raï','Reggae Fusion','Reggaeton','Romanian Etno','Salsa','Sertanejo',\n",
    " 'Riot Grrrl',\"Worship\",'Schlager','Ska','Bolero','Tango','Cabaret','Cajun','Celtic Fusion','Klezmer','Latin Rock','Soul',\n",
    " 'Space Rock','Symphonic Metal','Technical Death Metal','Tejano','Trance','Turkish Folk','Turkish Rock','Vocal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# go to each artist page, check whether genre constraints are met, and then fill the dictionary with necessary info\n",
    "# such as album name, album year, song name, lyrics, genre, etc. \n",
    "for artist, link in artist_to_url.items():\n",
    "    \n",
    "    lyrics_dict[artist] = [] # create an empty list for each artist in the dictionary, to be filled in later on\n",
    "    try:\n",
    "        page = urllib2.urlopen('https://lyrics.fandom.com'+link)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        \n",
    "        '''first get the genre and add it to the dictionary with the artist name'''\n",
    "        try: # see if you can get genres\n",
    "            genres = soup.find_all('div', {'class' : \"css-table-cell\"})\n",
    "            if genres == []: # if there are no genre blocks to be processed in the html, then we should proceed with the next artist\n",
    "                del lyrics_dict[artist]\n",
    "                continue\n",
    "            for item in genres:\n",
    "                try:\n",
    "                    b_blocks = item.find('b')\n",
    "                    for entry in b_blocks:\n",
    "                        if entry == 'Genres:':\n",
    "                            genre = item.find('a')['title'][15:]\n",
    "                            lyrics_dict[artist] = [genre, 'https://lyrics.fandom.com'+link, dict()] # create a list for each artist, consisting of the artist link, and the genre value     \n",
    "                except:\n",
    "                    pass\n",
    "        except:\n",
    "            pass\n",
    "        '''genre retrieved and saved'''\n",
    "        \n",
    "        \n",
    "        if (genre in avoid_genre_list): # only the desired genres are accepted\n",
    "            del lyrics_dict[artist]\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        '''then get the content for further classification'''\n",
    "        content = soup.find_all(\"span\", { \"class\" : \"mw-headline\" })\n",
    "        '''content retrieved'''\n",
    "        \n",
    "        '''here, to save time, we need to check whether the given artist has enough number of albums and songs \n",
    "        that are above a certain threshold. if not, we need to stop the process now, before consuming time on\n",
    "        accessing pages'''\n",
    "        album_count = 0\n",
    "        song_count = 0\n",
    "        for item in content:\n",
    "            if album_count >=3 and song_count >=10:\n",
    "                break\n",
    "            try:\n",
    "                album = item.find(\"a\", recursive=False)[\"title\"]\n",
    "                if album[-6:] != \"exist)\" and album[len(artist)] == ':':\n",
    "                    album_count +=1\n",
    "                    album_url = 'https://lyrics.fandom.com' + item.find(\"a\", recursive=False)[\"href\"]\n",
    "                    try:\n",
    "                        page = urllib2.urlopen(album_url)\n",
    "                        soup2 = BeautifulSoup(page, 'html.parser')\n",
    "                        song_list = soup2.find_all('b')\n",
    "                    except urllib2.HTTPError:\n",
    "                        pass\n",
    "                    for song in song_list:\n",
    "                        try:\n",
    "                            song_title = song.find('a', title = True, ref=False)['title'] # ref=False prevents listed songs without any links\n",
    "                            # when the title is not in the form \"Michael Jackson:...\", then there might be an additional artist in the song\n",
    "                            if song_title[len(artist)] == ':':\n",
    "                                song_count +=1\n",
    "                        except:\n",
    "                            pass\n",
    "            except:\n",
    "                pass\n",
    "        '''threshold check completed'''\n",
    "        \n",
    "        # we'll only continue if the artist has more than 3 albums and 10 songs with lyrics registered in the website\n",
    "        if album_count < 3 or song_count < 10:\n",
    "            del lyrics_dict[artist]\n",
    "            continue\n",
    "        else:        \n",
    "            '''if the threshold requirements are satisfied, continue with retrieving the content and storing it in the dictionary'''\n",
    "            album_count = 0\n",
    "            song_count = 0\n",
    "            for item in content:\n",
    "                try:\n",
    "                    album = item.find(\"a\", recursive=False)[\"title\"]\n",
    "                    if album[-6:] == \"exist)\" or album[len(artist)] != ':': #if the album name ends with (page does not exist), it means the album page is not there yet\n",
    "                        continue\n",
    "                    else:\n",
    "                        album_count += 1\n",
    "                        album_url = 'https://lyrics.fandom.com' + item.find(\"a\", recursive=False)[\"href\"]\n",
    "                        year = album[-5:-1]\n",
    "                        lyrics_dict[artist][-1][album] = [album_url, dict()]\n",
    "                        try:\n",
    "                            page = urllib2.urlopen(album_url)\n",
    "                            soup2 = BeautifulSoup(page, 'html.parser')\n",
    "                            song_list = soup2.find_all('b')\n",
    "                        except urllib2.HTTPError:\n",
    "                            pass\n",
    "                        for song in song_list:\n",
    "                            try:\n",
    "                                song_url = song.find('a', title = True, ref=False)['href'] # ref=False prevents listed songs without any links\n",
    "                                song_title = song.find('a', title = True, ref=False)['title']\n",
    "                                if song_title[len(artist)] == ':':\n",
    "                                    try: # get the lyrics of each song\n",
    "                                        lyricshtml = urllib2.urlopen('https://lyrics.fandom.com'+song_url)\n",
    "                                        lyricssoup = BeautifulSoup(lyricshtml, 'html.parser')\n",
    "                                        raw_lyrics = lyricssoup.find(attrs={'class': 'lyricbox'})\n",
    "                                    except urllib2.HTTPError:\n",
    "                                        pass\n",
    "                                    lyrics_dict[artist][-1][album][-1][song_title] = \\\n",
    "                                    [raw_lyrics.get_text(separator='<>'), year, song_url, str(id_count)]\n",
    "                                    song_count +=1\n",
    "                                    id_count +=1\n",
    "                                else:\n",
    "                                    continue\n",
    "                            except:\n",
    "                                pass\n",
    "                except:\n",
    "                    pass\n",
    "            # finally add the album and song count for each artist\n",
    "            lyrics_dict[artist].append(album_count)\n",
    "            lyrics_dict[artist].append(song_count)\n",
    "        \n",
    "            # here is a double check of the numbers.\n",
    "            # unless sufficient numbers are satisfied, we delete the artist data from the dict and continue\n",
    "            if album_count < 3 or song_count < 10:\n",
    "                del lyrics_dict[artist]\n",
    "                continue\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store everything  to a pickle file\n",
    "writePickle(lyrics_dict, 'lyrics_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
