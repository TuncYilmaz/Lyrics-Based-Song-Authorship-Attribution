{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Limit the size of the original POS songs to get a reduced dataset with fewer padding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by feeding your Pickle functions to call and save pickle variables later on\n",
    "\n",
    "import pickle\n",
    "def writePickle( Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "def readPickle(fname):\n",
    "    filename = \"pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "def readPicklefromPast(fname):\n",
    "    filename = \"../pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the previously formed POS datasets, prepared for training\n",
    "train_POS_input_data = readPickle(\"cnn_data_inputs/train_POS_input_data\")\n",
    "train_POS_labels = readPickle(\"cnn_data_inputs/train_POS_labels\")\n",
    "dev_POS_input_data = readPickle(\"cnn_data_inputs/dev_POS_input_data\")\n",
    "dev_POS_labels = readPickle(\"cnn_data_inputs/dev_POS_labels\")\n",
    "test_POS_input_data = readPickle(\"cnn_data_inputs/test_POS_input_data\")\n",
    "test_POS_labels = readPickle(\"cnn_data_inputs/test_POS_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def POS_refiner(input_data):\n",
    "    new_input_data = list()\n",
    "    for song in input_data:\n",
    "        reduced_song = []\n",
    "        song_line_ruler = list()\n",
    "        for line in song:\n",
    "            if len(list(filter(lambda a: a != 0, line))) != 0:\n",
    "                new_line = list(filter(lambda a: a != 0, line))\n",
    "                song_line_ruler.append((song.index(line),len(new_line)))\n",
    "            else:\n",
    "                song_line_ruler.append((song.index(line),0))\n",
    "        song_line_ruler = list(set(song_line_ruler)) # remove duplicate lines\n",
    "        song_line_ruler.sort(key=lambda x: x[1], reverse=True) # sort by the number of POS tags present\n",
    "        song_line_ruler = song_line_ruler[0:20] # take the most frequent 20\n",
    "        song_line_ruler.sort(key=lambda x: x[0]) # sort them back in shape wrt their original places in the song\n",
    "\n",
    "        for item in song_line_ruler:\n",
    "            reduced_song.append(song[item[0]])\n",
    "        refined_song = list()\n",
    "        for line in reduced_song:\n",
    "            if set(line) <= {0}: # if the line is all blanks, replace the line with ten random integers up to 18\n",
    "                refined_song.append([random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18)])\n",
    "            else:\n",
    "                refined_line = list(filter(lambda a: a != 0, line))\n",
    "                if len(refined_line) > 10: # if there are more than ten elements, take the last 10 of them\n",
    "                    refined_line = refined_line[-10:]\n",
    "                else: # if there are only less than 10 elements\n",
    "                    diff = 10 - len(refined_line)\n",
    "                    for _ in range(diff): # randomly fill POS tags in \n",
    "                        refined_line.insert(random.randrange(0,len(refined_line),1),random.randint(0,18))\n",
    "                refined_song.append(refined_line)\n",
    "        if len(refined_song) < 20:\n",
    "            diff = 20 - len(refined_song)\n",
    "            for _ in range(diff):\n",
    "                blank_line = [random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18),random.randint(0,18)]\n",
    "                refined_song.append(blank_line)\n",
    "            \n",
    "        new_input_data.append(refined_song)\n",
    "    return new_input_data\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_POS_reduced_input_data = POS_refiner(train_POS_input_data)\n",
    "dev_POS_reduced_input_data = POS_refiner(dev_POS_input_data)\n",
    "test_POS_reduced_input_data = POS_refiner(test_POS_input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert all of them to numpy arrays, so that they can be used in keras\n",
    "import numpy as np\n",
    "\n",
    "train_POS_input_data = np.array(train_POS_reduced_input_data)\n",
    "train_POS_labels = np.array(train_POS_labels)\n",
    "dev_POS_input_data = np.array(dev_POS_reduced_input_data)\n",
    "dev_POS_labels = np.array(dev_POS_labels)\n",
    "test_POS_input_data = np.array(test_POS_reduced_input_data)\n",
    "test_POS_labels = np.array(test_POS_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all the input data, we have to normalize the data points to an interval between 0 and 1, \n",
    "# and convert everything to floating numbers\n",
    "train_POS_input_data = train_POS_input_data.astype('float32') / np.amax(train_POS_input_data)\n",
    "dev_POS_input_data = dev_POS_input_data.astype('float32') / np.amax(dev_POS_input_data)\n",
    "test_POS_input_data = test_POS_input_data.astype('float32') / np.amax(test_POS_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an example\n",
    "train_POS_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape input\n",
    "\n",
    "X_train_POS = train_POS_input_data.reshape(len(train_POS_input_data),20,10,1)\n",
    "X_dev_POS = dev_POS_input_data.reshape(len(dev_POS_input_data),20,10,1)\n",
    "X_test_POS = test_POS_input_data.reshape(len(test_POS_input_data),20,10,1)\n",
    "\n",
    "# an example\n",
    "print(X_train_POS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then we need to turn the labels into categorical values\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_POS = to_categorical(train_POS_labels)\n",
    "y_dev_POS = to_categorical(dev_POS_labels)\n",
    "y_test_POS = to_categorical(test_POS_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# record all of them to pickle files\n",
    "writePickle(X_train_POS,\"cnn_data_inputs/POS_Keras/X_train_POS_reduced\")\n",
    "writePickle(X_dev_POS,\"cnn_data_inputs/POS_Keras/X_dev_POS_reduced\")\n",
    "writePickle(X_test_POS,\"cnn_data_inputs/POS_Keras/X_test_POS_reduced\")\n",
    "writePickle(y_train_POS,\"cnn_data_inputs/POS_Keras/y_train_POS_reduced\")\n",
    "writePickle(y_dev_POS,\"cnn_data_inputs/POS_Keras/y_dev_POS_reduced\")\n",
    "writePickle(y_test_POS,\"cnn_data_inputs/POS_Keras/y_test_POS_reduced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the this moment on, we will use the Model scripts to construct our models, using the pickle input variables right above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
