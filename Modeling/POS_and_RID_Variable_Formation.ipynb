{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here, we will make the final conversion of the lyrics (written in POS tags and RID tags) to a format that suits CNN requirements. We'll follow the steps below:\n",
    "- By using a script named 'Lyrics2POSandRID_Converter.py', we will convert each song in the final collection to POS tags and RID tags. Then the converted versions will be stored under two distinct dictionaries, one mapping artists to a list of songs written in POS tags, and the other in RID tags\n",
    "- We will generate two sets (one for POS and one for RID) that contain the unique tags including 'PADDING'. Each of these tags will be mapped to a unique index number\n",
    "- For future efficiency, each song id will be mapped to its POS_Lyric and RID_Lyric version in two separate dictionaries\n",
    "- Each song should be extended to a size of 100 lines and 20 tokens per line. Therefore we will take each song and fill in the blanks with 'PADDING' until all of them has size (100x20)\n",
    "- The padded songs will be converted to tag indices, so that they can be used in the CNN. Then, these indices will be normalized to numbers between 0 and 1 for CNN calculation efficiency.\n",
    "- We'll terminate the session by using the training-test-development split that is already prepared in a separate file called 'Training-Development-Test Set Allocation.ipynb'. These splitted datasets will be stored and sent to the scripts that constructs our CNN models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start by writing the Pickle functions to call and save pickle variables later on\n",
    "\n",
    "import pickle\n",
    "def writePickle( Variable, fname):\n",
    "    filename = fname +\".pkl\"\n",
    "    f = open(\"pickle_vars/\"+filename, 'wb')\n",
    "    pickle.dump(Variable, f)\n",
    "    f.close()\n",
    "def readPickle(fname):\n",
    "    filename = \"pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj\n",
    "def readPicklefromPast(fname):\n",
    "    filename = \"../pickle_vars/\"+fname +\".pkl\"\n",
    "    f = open(filename, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the POS and RID lyrics dictionaries generated in 'Lyrics2POSandRID_Converter.py'\n",
    "final_artists_to_RIDsongs_dict = readPickle(\"final_artists_to_RIDsongs_dict\")\n",
    "final_artists_to_POSsongs_dict = readPickle(\"final_artists_to_POSsongs_dict\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a set that contains all the unique POS tags used by spaCy\n",
    "unique_POS_set = set()\n",
    "for artist, song_list in final_artists_to_POSsongs_dict.items():\n",
    "    for song in song_list:\n",
    "        flattened_POS_set = set([POS for line in song for POS in line])\n",
    "        unique_POS_set.update(flattened_POS_set)\n",
    "print(unique_POS_set)\n",
    "\n",
    "# do the same for the RID tags\n",
    "# First create a set that contains all the unique RID tags used within the songs\n",
    "unique_RID_set = set()\n",
    "for artist, song_list in final_artists_to_RIDsongs_dict.items():\n",
    "    for song in song_list:\n",
    "        flattened_RID_set = set([RID for line in song for RID in line])\n",
    "        unique_RID_set.update(flattened_RID_set)\n",
    "print(unique_RID_set)\n",
    "\n",
    "# add 'PADDING' as the first element to the list versions of these sets\n",
    "unique_POS_list = list(unique_POS_set)\n",
    "unique_POS_list.insert(0,\"PADDING\")\n",
    "print(unique_POS_list)\n",
    "\n",
    "unique_RID_list = list(unique_RID_set)\n",
    "unique_RID_list.insert(0,\"PADDING\")\n",
    "print(unique_RID_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then link every RID tag and POS tag respectively to a number and store these links in both directions in two dictionaries\n",
    "RID2id = {rid: index for index, rid in enumerate(unique_RID_list)}\n",
    "id2RID = {index: rid for index, rid in enumerate(unique_RID_list)}\n",
    "print(RID2id)\n",
    "\n",
    "POS2id = {pos: index for index, pos in enumerate(unique_POS_list)}\n",
    "id2POS = {index: pos for index, pos in enumerate(unique_POS_list)}\n",
    "print(POS2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# also the artist names should be mapped to numbers\n",
    "artist_names = list(final_artists_to_RIDsongs_dict.keys())\n",
    "Artist2id = {artist: index for index, artist in enumerate(artist_names)}\n",
    "id2Artist = {index: artist for index, artist in enumerate(artist_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store all these variables under the folder 'indexing'\n",
    "writePickle(POS2id, \"indexing/POS2id\")\n",
    "writePickle(id2POS, \"indexing/id2POS\")\n",
    "writePickle(RID2id, \"indexing/RID2id\")\n",
    "writePickle(id2RID, \"indexing/id2RID\")\n",
    "writePickle(Artist2id, \"indexing/Artist2id\")\n",
    "writePickle(id2Artist, \"indexing/id2Artist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with creating dictionaries that contain song ids mapped to POS lyrics and RID lyrics and actual lyrics respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_IDs_to_Lyrics_dict = dict()\n",
    "final_IDs_to_POS_dict = dict()\n",
    "final_IDs_to_RID_dict = dict()\n",
    "\n",
    "for artist, rid_songs in final_artists_to_RIDsongs_dict.items():\n",
    "    song_IDs = final_constrained_artist2idlist_dict[artist]\n",
    "    song_lyrics = final_artist2lyrics_dict[artist]\n",
    "    for songID, RID in zip(song_IDs,rid_songs):\n",
    "        final_IDs_to_RID_dict[songID] = RID\n",
    "    for songID, lyrics in zip(song_IDs,song_lyrics):\n",
    "        final_IDs_to_Lyrics_dict[songID] = lyrics\n",
    "    \n",
    "for artist, pos_songs in final_artists_to_POSsongs_dict.items():\n",
    "    song_IDs = final_constrained_artist2idlist_dict[artist]    \n",
    "    for songID, POS in zip(song_IDs,pos_songs):\n",
    "        final_IDs_to_POS_dict[songID] = POS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print the same song from each of the lists to see similarities\n",
    "print(final_IDs_to_POS_dict[\"241007\"], len(list(final_IDs_to_POS_dict.keys())))\n",
    "print(final_IDs_to_RID_dict[\"241007\"], len(list(final_IDs_to_RID_dict.keys())))\n",
    "print(final_IDs_to_Lyrics_dict[\"241007\"], len(list(final_IDs_to_Lyrics_dict.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# record these dictionaries into pickle files\n",
    "writePickle(final_IDs_to_POS_dict, 'final_IDs_to_POS_dict')\n",
    "writePickle(final_IDs_to_RID_dict, 'final_IDs_to_RID_dict')\n",
    "writePickle(final_IDs_to_Lyrics_dict, 'final_IDs_to_Lyrics_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here you can plot a couple of histograms to see the song length and line length distributions over the dataset for POS tags\n",
    "song_len_list = list()\n",
    "line_len_list = list()\n",
    "for song in final_IDs_to_POS_dict.values():\n",
    "    song_len_list.append(len(song))\n",
    "    for line in song:\n",
    "        line_len_list.append(len(line))\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "all_line_freq = defaultdict(int)\n",
    "for len in line_len_list:\n",
    "    all_line_freq[len] += 1\n",
    "all_song_freq = defaultdict(int)\n",
    "for len in song_len_list:\n",
    "    all_song_freq[len] += 1\n",
    "plt.bar(all_line_freq.keys(), all_line_freq.values(), width=1, color='g')\n",
    "plt.show()  \n",
    "plt.bar(all_song_freq.keys(), all_song_freq.values(), width=1, color='g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each song should be extended to a size of 100 lines and 20 tokens per line. Therefore we will take each song and fill in the blanks with 'PADDING' until all of them has size (100x20). <br>\n",
    "Also each song should be converted to its indexed version <br>\n",
    "Finally, the converted datasets should be splitted into training, development and test sets<br>\n",
    "The following cell script handles these steps: <br>\n",
    "(we use the previously created variables here. for the generation of train-dev-test split that are provided in \"train_df\", \"dev-def\" and \"test_df\" variables respectively, please refer to the separate notebook file named as 'Training-Development-Test Set Allocation.ipynb') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "max_song = 100 # maximum song length\n",
    "max_line = 20 # maximum number of tokens in a line\n",
    "\n",
    "RID2id = readPickle(\"indexing/RID2id\")\n",
    "POS2id = readPickle(\"indexing/POS2id\")\n",
    "Artist2id = readPickle(\"indexing/Artist2id\")\n",
    "ID_to_RID = readPickle(\"final_IDs_to_RID_dict\")\n",
    "ID_to_POS = readPickle(\"final_IDs_to_POS_dict\")\n",
    "\n",
    "# import also the splitted datasets\n",
    "train_df = readPickle(\"train_df\")\n",
    "dev_df = readPickle(\"dev_df\")\n",
    "test_df = readPickle(\"test_df\")\n",
    "\n",
    "# start with the RID generator function\n",
    "def RID_generator(dataframe): # pick any of the dataframes; -test, -train or -dev\n",
    "    RID_dict = dataframe.to_dict('list')\n",
    "    sorted_RID_dict = OrderedDict(sorted(RID_dict.items(), key=lambda v: v, reverse=True))\n",
    "    artists = list()\n",
    "    songs = list()\n",
    "    for artist, song_ID_list in sorted_RID_dict.items():\n",
    "        for song_ID in song_ID_list:\n",
    "            RID_song = list()\n",
    "            artists.append(Artist2id[artist])\n",
    "            song = ID_to_RID[song_ID]\n",
    "            while len(song) != max_song:\n",
    "                song.append([\"PADDING\"])\n",
    "            for line in song:\n",
    "                while len(line) != max_line:\n",
    "                    line.append(\"PADDING\")\n",
    "            for line in song:\n",
    "                RID_line = list()\n",
    "                for tag in line:\n",
    "                    RID_line.append(RID2id[tag])\n",
    "                RID_song.append(RID_line)\n",
    "            songs.append(RID_song)\n",
    "    return songs, artists\n",
    "\n",
    "# using the function, form the datasets in python list format\n",
    "train_RID_input_data, train_RID_labels = RID_generator(train_df)\n",
    "print(\"Training data finished for RID, continuing with development data...\")\n",
    "dev_RID_input_data, dev_RID_labels = RID_generator(dev_df)\n",
    "print(\"Development data finished for RID, continuing with test data...\")\n",
    "test_RID_input_data, test_RID_labels = RID_generator(test_df)\n",
    "print(\"Test data finished for RID, continuing with POS generation...\")\n",
    "\n",
    "# replicate the process for the POS tags\n",
    "def POS_generator(dataframe): # pick any of the dataframes; -test, -train or -dev\n",
    "    POS_dict = dataframe.to_dict('list')\n",
    "    sorted_POS_dict = OrderedDict(sorted(POS_dict.items(), key=lambda v: v, reverse=True))\n",
    "    artists = list()\n",
    "    songs = list()\n",
    "    for artist, song_ID_list in sorted_POS_dict.items():\n",
    "        for song_ID in song_ID_list:\n",
    "            POS_song = list()\n",
    "            artists.append(Artist2id[artist])\n",
    "            song = ID_to_POS[song_ID]\n",
    "            while len(song) != max_song:\n",
    "                song.append([\"PADDING\"])\n",
    "            for line in song:\n",
    "                while len(line) != max_line:\n",
    "                    line.append(\"PADDING\")\n",
    "            for line in song:\n",
    "                #print(\"line is\",line)\n",
    "                POS_line = list()\n",
    "                for tag in line:\n",
    "                    #print(tag)\n",
    "                    POS_line.append(POS2id[tag])\n",
    "                POS_song.append(POS_line)\n",
    "            songs.append(POS_song)\n",
    "    return songs, artists\n",
    "\n",
    "# using the function, form the datasets in python list format\n",
    "train_POS_input_data, train_POS_labels = POS_generator(train_df)\n",
    "print(\"Training data finished for POS, continuing with development data...\")\n",
    "dev_POS_input_data, dev_POS_labels = POS_generator(dev_df)\n",
    "print(\"Development data finished for POS, continuing with test data...\")\n",
    "test_POS_input_data, test_POS_labels = POS_generator(test_df)\n",
    "print(\"Test data finished for POS, continuing with pickle file recording...\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# in the end store these as pickle variables for later use\n",
    "writePickle(train_POS_input_data, \"cnn_data_inputs/train_POS_input_data\")\n",
    "writePickle(train_POS_labels, \"cnn_data_inputs/train_POS_labels\")\n",
    "writePickle(dev_POS_input_data, \"cnn_data_inputs/dev_POS_input_data\")\n",
    "writePickle(dev_POS_labels, \"cnn_data_inputs/dev_POS_labels\")\n",
    "writePickle(test_POS_input_data, \"cnn_data_inputs/test_POS_input_data\")\n",
    "writePickle(test_POS_labels, \"cnn_data_inputs/test_POS_labels\")\n",
    "\n",
    "writePickle(train_RID_input_data, \"cnn_data_inputs/train_RID_input_data\")\n",
    "writePickle(train_RID_labels, \"cnn_data_inputs/train_RID_labels\")\n",
    "writePickle(dev_RID_input_data, \"cnn_data_inputs/dev_RID_input_data\")\n",
    "writePickle(dev_RID_labels, \"cnn_data_inputs/dev_RID_labels\")\n",
    "writePickle(test_RID_input_data, \"cnn_data_inputs/test_RID_input_data\")\n",
    "writePickle(test_RID_labels, \"cnn_data_inputs/test_RID_labels\")\n",
    "\n",
    "\n",
    "print(\"An example of training RID input data is:\", train_RID_input_data[0])\n",
    "print(\"The first training RID label is\", train_RID_labels[0])\n",
    "print(\"The first training POS label is\", train_POS_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have obtained all 6 datasets (2 each for train, test and dev, 1 for inputs and 1 for labels) for both POS and RID songs. It is time to place these into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start with reading these dataset variables from pickle files\n",
    "train_POS_input_data = readPickle(\"cnn_data_inputs/train_POS_input_data\")\n",
    "train_POS_labels = readPickle(\"cnn_data_inputs/train_POS_labels\")\n",
    "dev_POS_input_data = readPickle(\"cnn_data_inputs/dev_POS_input_data\")\n",
    "dev_POS_labels = readPickle(\"cnn_data_inputs/dev_POS_labels\")\n",
    "test_POS_input_data = readPickle(\"cnn_data_inputs/test_POS_input_data\")\n",
    "test_POS_labels = readPickle(\"cnn_data_inputs/test_POS_labels\")\n",
    "\n",
    "train_RID_input_data = readPickle(\"cnn_data_inputs/train_RID_input_data\")\n",
    "train_RID_labels = readPickle(\"cnn_data_inputs/train_RID_labels\")\n",
    "dev_RID_input_data = readPickle(\"cnn_data_inputs/dev_RID_input_data\")\n",
    "dev_RID_labels = readPickle(\"cnn_data_inputs/dev_RID_labels\")\n",
    "test_RID_input_data = readPickle(\"cnn_data_inputs/test_RID_input_data\")\n",
    "test_RID_labels = readPickle(\"cnn_data_inputs/test_RID_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert all of them to numpy arrays, so that they can be used in keras\n",
    "import numpy as np\n",
    "\n",
    "train_POS_input_data = np.array(train_POS_input_data)\n",
    "train_POS_labels = np.array(train_POS_labels)\n",
    "dev_POS_input_data = np.array(dev_POS_input_data)\n",
    "dev_POS_labels = np.array(dev_POS_labels)\n",
    "test_POS_input_data = np.array(test_POS_input_data)\n",
    "test_POS_labels = np.array(test_POS_labels)\n",
    "\n",
    "train_RID_input_data = np.array(train_RID_input_data)\n",
    "train_RID_labels = np.array(train_RID_labels)\n",
    "dev_RID_input_data = np.array(dev_RID_input_data)\n",
    "dev_RID_labels = np.array(dev_RID_labels)\n",
    "test_RID_input_data = np.array(test_RID_input_data)\n",
    "test_RID_labels = np.array(test_RID_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see an example\n",
    "train_POS_input_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for all the input data, we have to normalize the data points to an interval between 0 and 1, \n",
    "# and convert everything to floating numbers\n",
    "\n",
    "print(np.amax(train_POS_input_data))\n",
    "print(np.amax(test_POS_input_data))\n",
    "print(np.amax(dev_POS_input_data))\n",
    "\n",
    "print(np.amax(train_RID_input_data))\n",
    "print(np.amax(test_RID_input_data))\n",
    "print(np.amax(dev_RID_input_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_POS_input_data = train_POS_input_data.astype('float32') / np.amax(train_POS_input_data)\n",
    "dev_POS_input_data = dev_POS_input_data.astype('float32') / np.amax(dev_POS_input_data)\n",
    "test_POS_input_data = test_POS_input_data.astype('float32') / np.amax(test_POS_input_data)\n",
    "\n",
    "train_RID_input_data = train_RID_input_data.astype('float32') / np.amax(train_RID_input_data)\n",
    "dev_RID_input_data = dev_RID_input_data.astype('float32') / np.amax(dev_RID_input_data)\n",
    "test_RID_input_data = test_RID_input_data.astype('float32') / np.amax(test_RID_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see an example\n",
    "print(train_POS_input_data[0])\n",
    "print(train_POS_labels[0])\n",
    "print(train_POS_input_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reshape the inputs into desired format\n",
    "\n",
    "X_train_POS = train_POS_input_data.reshape(len(train_POS_input_data),max_song,max_line,1)\n",
    "X_dev_POS = dev_POS_input_data.reshape(len(dev_POS_input_data),max_song,max_line,1)\n",
    "X_test_POS = test_POS_input_data.reshape(len(test_POS_input_data),max_song,max_line,1)\n",
    "\n",
    "X_train_RID = train_RID_input_data.reshape(len(train_RID_input_data),max_song,max_line,1)\n",
    "X_dev_RID = dev_RID_input_data.reshape(len(dev_RID_input_data),max_song,max_line,1)\n",
    "X_test_RID = test_RID_input_data.reshape(len(test_RID_input_data),max_song,max_line,1)\n",
    "\n",
    "# check the final shapes\n",
    "print(X_train_POS.shape)\n",
    "print(X_train_RID.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# turn the labels into categorical values\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "y_train_POS = to_categorical(train_POS_labels)\n",
    "y_dev_POS = to_categorical(dev_POS_labels)\n",
    "y_test_POS = to_categorical(test_POS_labels)\n",
    "\n",
    "y_train_RID = to_categorical(train_RID_labels)\n",
    "y_dev_RID = to_categorical(dev_RID_labels)\n",
    "y_test_RID = to_categorical(test_RID_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see how it works\n",
    "print(y_train_RID)\n",
    "print(y_train_POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record all of the formatted final input and outputs into pickle files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the variables\n",
    "writePickle(X_train_POS,\"cnn_data_inputs/POS_Keras/X_train_POS\")\n",
    "writePickle(X_dev_POS,\"cnn_data_inputs/POS_Keras/X_dev_POS\")\n",
    "writePickle(X_test_POS,\"cnn_data_inputs/POS_Keras/X_test_POS\")\n",
    "writePickle(y_train_POS,\"cnn_data_inputs/POS_Keras/y_train_POS\")\n",
    "writePickle(y_dev_POS,\"cnn_data_inputs/POS_Keras/y_dev_POS\")\n",
    "writePickle(y_test_POS,\"cnn_data_inputs/POS_Keras/y_test_POS\")\n",
    "\n",
    "writePickle(X_train_RID,\"cnn_data_inputs/RID_Keras/X_train_RID\")\n",
    "writePickle(X_dev_RID,\"cnn_data_inputs/RID_Keras/X_dev_RID\")\n",
    "writePickle(X_test_RID,\"cnn_data_inputs/RID_Keras/X_test_RID\")\n",
    "writePickle(y_train_RID,\"cnn_data_inputs/RID_Keras/y_train_RID\")\n",
    "writePickle(y_dev_RID,\"cnn_data_inputs/RID_Keras/y_dev_RID\")\n",
    "writePickle(y_test_RID,\"cnn_data_inputs/RID_Keras/y_test_RID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the this moment on, we will use our Model scripts to construct our models, using the inputs prepared and stored in pickle files above. For an example model script, please check \"POS_Model.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
